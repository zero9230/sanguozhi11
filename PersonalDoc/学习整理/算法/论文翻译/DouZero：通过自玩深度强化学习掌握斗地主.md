# 1 摘要
游戏是现实世界的抽象，人工代理在其中学习与其他代理竞争和合作。尽管在各种完美信息和不完美信息游戏中取得了显著成绩，但三人卡牌游戏斗地主（又名斗地主）仍未解决。斗地主是一个非常具有挑战性的领域，具有竞争、协作、不完善的信息、大的状态空间，尤其是大量可能的行为，其中法律行为在各个回合之间存在显着差异。不幸的是，现代强化学习算法主要集中在简单和小的动作空间上，不足为奇的是，在斗地主中没有取得令人满意的进展。在这项工作中，我们提出了一个概念上简单但有效的斗地主人工智能系统，即 DouZero，它增强了传统的蒙特卡罗方法
具有深度神经网络、动作编码和并行参与者。从零开始，在具有四个 GPU 的单个服务器上，DouZero 在几天的训练中超越了所有现有的 DouDizhu AI 程序，并在 Botzone 排行榜中排名 344 个 AI 代理的第一名。通过构建 DouZero，我们展示了经典的 Monte-Carlo 方法可以在具有复杂动作空间的硬域中提供强大的结果。代码和在线演示已发布1，希望这种见解可以激发未来的工作。

# 2 引言
游戏通常作为人工智能的基准，因为它们是许多现实世界问题的抽象。在完全信息博弈中取得了显著成就。例如，AlphaGo (Silver et al., 2016)、AlphaZero (Silver et al., 2018) 和 MuZero (Schrittwieser et al., 2020) 在围棋游戏中建立了最先进的性能。最近的研究已经发展到更具挑战性的不完全信息博弈，其中代理在部分可观察的环境中与他人竞争或合作。两人游戏取得了令人鼓舞的进展，例如简单的 Leduc Hold'em 和有限/无限德州 Hold'em (Zinkevich et al., 2008; Heinrich & Silver, 2016; Moravcˇ́ık et al., 2017; Brown & Sandholm, 2018)，多人游戏，例如多人德州扑克 (Brown & Sandholm, 2019b)、星际争霸 (Vinyals et al., 2019)、DOTA (Berner et al., 2019) )、Hanabi (Lerer et al., 2020)、麻将 (Li et al., 2020a)、王者荣耀 (Ye et al., 2020b;a) 和无压力外交 (Gray et al., 2020)。
这项工作旨在为斗地主2（又名斗地主）构建人工智能程序，这是中国最受欢迎的纸牌游戏，拥有数亿日活跃玩家。斗地主有两个有趣的特性
人工智能系统面临的挑战。首先，DouDizhu 中的玩家需要在一个部分可观察的环境中与他人竞争和合作，沟通有限。具体来说，两名农民玩家将组队对抗地主玩家。流行的扑克游戏算法，例如反事实遗憾最小化 (CFR) (Zinkevich et al., 2008)）及其变体，在这种复杂的三人游戏环境中通常并不可靠。其次，斗地主拥有大量平均规模非常大的信息集，并且由于卡片的组合而具有多达 104 个可能的动作的非常复杂和大的动作空间（Zha 等，2019a）。与德州扑克不同，斗地注中的动作不能轻易抽象，这使得搜索计算量大，常用的强化学习算法效率低下。由于高估问题，深度 Q-Learning (DQN) (Mnih et al., 2015) 在非常大的动作空间中存在问题 (Zahaviy et al., 2018)；策略梯度方法，例如 A3C (Mnih et al., 2016)，不能利用 DouDizhu 中的动作特征，因此不能像 DQN 那样自然地泛化看不见的动作 (Dulac-Arnold et al., 2015)。毫不奇怪，之前的工作表明DQN和A3C在斗地主方面并不能取得令人满意的进展。在 (You et al., 2019) 中，即使经过 20 天的训练，DQN 和 A3C 对简单的基于规则的代理的胜率也低于 20%； (Zha et al., 2019a) 中的 DQN 仅略好于对合法移动进行均匀采样的随机代理。

之前已经做出了一些努力，通过将人类启发式与学习和搜索相结合来构建斗地主 AI。组合 Q-Network (CQN) (You et al., 2019) 提出通过将动作解耦为分解选择和最终移动选择来减少动作空间。然而，分解依赖于人类的启发式方法并且非常缓慢。在实践中，经过 20 天的训练，CQN 甚至无法击败简单的启发式规则。 DeltaDou (Jiang et al., 2019) 是第一个与顶级人类玩家相比达到人类水平性能的 AI 程序。它通过使用贝叶斯方法来推断隐藏信息并根据他们自己的策略网络对其他玩家的行为进行采样，从而实现了类似于 AlphaZero 的算法。为了抽象动作空间，DeltaDou 基于启发式规则预训练了一个 kicker 网络。然而，踢球者在斗地注中起着重要的作用，不能轻易抽象。踢球者选择不当可能会直接导致输掉比赛，因为它可能会破坏其他一些卡牌类别，例如单人链。此外，贝叶斯推理和搜索在计算上是昂贵的。即使使用启发式的监督回归初始化网络，训练 DeltaDou 也需要两个多月的时间（Jiang et al., 2019）。因此，现有的斗地主 AI 程序在计算上非常昂贵，并且可能不是最优的，因为它们高度依赖于人类知识的抽象。
在这项工作中，我们介绍了 DouZero，这是一个概念上简单但有效的用于 DouDizhu 的 AI 系统，无需抽象状态/动作空间或任何人类知识。 DouZero 通过深度神经网络、动作编码和并行参与者增强了传统的 Monte-Carlo 方法 (Sutton & Barto, 2018)。 DouZero 有两个理想的属性。首先，与 DQN 不同，它不易受到过度估计偏差的影响。其次，通过将动作编码成卡片矩阵，它可以自然地泛化在整个训练过程中不常见的动作。这两个属性对于处理斗地主庞大而复杂的动作空间至关重要。与许多树搜索算法不同，DouZero 是基于采样的，它允许我们使用复杂的神经架构并在相同的计算资源的情况下每秒生成更多的数据。与许多依赖领域特定抽象的先前扑克 AI 研究不同，DouZero 不需要任何领域知识或底层动态知识。在只有 48 个内核和 4 个 1080Ti GPU 的单台服务器上从头开始训练，DouZero 在半天的时间内超过了 CQN 和启发式规则，在两天内击败了我们的内部监督代理，在十天内超过了 DeltaDou。广泛的评估表明，斗零是迄今为止最强的斗地主人工智能系统。

![](DouZero：通过自玩深度强化学习掌握斗地主.assets/image-20221021123101051.png)
图 1. 一手牌及其相应的合法移动。

通过构建 DouZero 系统，我们证明了经典的 Monte-Carlo 方法可以在需要对巨大状态和动作空间的竞争和合作进行推理的大型复杂纸牌游戏中提供强大的结果。我们注意到，一些工作还发现蒙特卡罗方法可以实现有竞争力的表现（Mania 等人，2018；Zha 等人，2021a）并有助于稀疏奖励设置（Guo 等人，2018；Zha 等人， 2021b)。与这些专注于简单和小型环境的研究不同，我们展示了蒙特卡洛方法在大型纸牌游戏中的强大性能。希望这种见解可以促进未来关于解决多智能体学习、稀疏奖励、复杂动作空间和不完善信息的研究，我们发布了我们的环境和训练代码。与许多需要数千个 CPU 进行训练的扑克 AI 系统不同，例如 DeepStack (Moravcˇ ́ık et al., 2017) 和 Libratus (Brown & Sandholm, 2018)，DouZero 实现了合理的实验流程，只需要在大多数研究实验室都负担得起的单 GPU 服务器。我们希望它可以激发该领域的未来研究，并作为一个强有力的基线。
